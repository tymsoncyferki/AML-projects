{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import bernoulli, multivariate_normal\n",
    "from scipy.sparse import csr_matrix\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, missing_threshold=0.0, correlation_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Preprocess dataset by:\n",
    "    1. Filling in missing values\n",
    "    2. Removing collinear variables\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - missing_threshold: maximum fraction of missing values allowed in a column\n",
    "    - correlation_threshold: threshold for removing correlated features\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    \n",
    "    # 1. Handle missing values\n",
    "    # Count missing values per column\n",
    "    missing_count = df.isnull().sum()\n",
    "    print(f\"Columns with missing values: {sum(missing_count > 0)}\")\n",
    "    \n",
    "    # Remove columns with too many missing values\n",
    "    cols_to_drop = missing_count[missing_count > missing_threshold * len(df)].index\n",
    "    print(f\"Dropping {len(cols_to_drop)} columns with >={missing_threshold*100}% missing values\")\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # For remaining columns with missing values, fill with median (numerical)\n",
    "    cols_to_fill = df.columns[df.isnull().any()]\n",
    "    for col in cols_to_fill:\n",
    "        if df[col].dtype.kind in 'ifc':  # Check if column is numeric\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        else:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            \n",
    "    # 2. Remove collinear variables\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df.drop(columns='target').corr().abs()\n",
    "    \n",
    "    # Create upper triangle mask\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find features with correlation above threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > correlation_threshold)]\n",
    "    print(f\"Dropping {len(to_drop)} collinear features with correlation > {correlation_threshold}\")\n",
    "    \n",
    "    # Drop collinear features\n",
    "    df = df.drop(columns=to_drop)\n",
    "    \n",
    "    print(f\"Final shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arhythmia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"raw_data/arrhythmia/arrhythmia.data\", header=None, na_values='?')\n",
    "\n",
    "# set last column as target\n",
    "data.rename(columns={data.columns[-1]: 'target'}, inplace=True)\n",
    "\n",
    "# convert classes to binary: 0 (Normal), 1 (all others)\n",
    "data['target'] = data['target'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# create data directory if needed\n",
    "os.makedirs('data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (452, 280)\n",
      "Columns with missing values: 5\n",
      "Dropping 5 columns with >=0.0% missing values\n",
      "Dropping 60 collinear features with correlation > 0.8\n",
      "Final shape: (452, 215)\n"
     ]
    }
   ],
   "source": [
    "data = preprocess_data(data, missing_threshold=0.0, correlation_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'data/arrhythmia.csv'.\n"
     ]
    }
   ],
   "source": [
    "# save data to CSV\n",
    "data.to_csv('data/arrhythmia.csv', index=False)\n",
    "print(\"Data saved to 'data/arrhythmia.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"raw_data/speech/pd_speech_features.csv\").drop(columns=['id'])\n",
    "data.rename(columns={data.columns[-1]: 'target'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.drop(columns=['target']).corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "data.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "binary_cols = X.nunique()[X.nunique() == 2].index\n",
    "numeric_cols = X.columns.difference(binary_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = {col: i for i, col in enumerate(data.columns[:-1])}  \n",
    "data = data.rename(columns=new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (756, 390)\n",
      "Columns with missing values: 0\n",
      "Dropping 0 columns with >=0.0% missing values\n",
      "Dropping 91 collinear features with correlation > 0.8\n",
      "Final shape: (756, 299)\n"
     ]
    }
   ],
   "source": [
    "data = preprocess_data(data, missing_threshold=0.0, correlation_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/speech.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' https://archive.ics.uci.edu/dataset/179/secom '"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" https://archive.ics.uci.edu/dataset/179/secom \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_secom = pd.read_csv(\"raw_data/secom/secom.data\", header=None, sep=' ')\n",
    "y_secom = pd.read_csv(\"raw_data/secom/secom_labels.data\", header=None, sep=' ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_secom = y_secom.apply(lambda x: max(x,0))  # change -1 to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nans\n",
    "X_secom = X_secom.fillna(X_secom.mean())\n",
    "\n",
    "# drop correlated columns\n",
    "tr = DropCorrelatedFeatures(None, threshold=0.9)\n",
    "X_secom = tr.fit_transform(X_secom)\n",
    "X_secom = pd.DataFrame(X_secom)\n",
    "\n",
    "# drop columns with only one unique value\n",
    "X_secom = X_secom.loc[:, X_secom.nunique() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate\n",
    "newdata = X_secom.copy()\n",
    "newdata['target'] = y_secom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance dataset\n",
    "counts = y_secom.value_counts()\n",
    "diff = counts[0] - counts[1]\n",
    "\n",
    "subset = newdata.drop(newdata[newdata['target'] == 0].sample(n=diff, random_state=42).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instances x features: (208, 268)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    104\n",
       "0    104\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_subset = subset[\"target\"]\n",
    "X_subset = subset.drop(columns=[\"target\"])\n",
    "\n",
    "print(\"instances x features:\", X_subset.shape)\n",
    "y_subset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.to_csv(\"data/secom.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (351, 35)\n",
      "Class distribution:\n",
      "34\n",
      "g    225\n",
      "b    126\n",
      "Name: count, dtype: int64\n",
      "Label mapping: {'g': 0, 'b': 1}\n"
     ]
    }
   ],
   "source": [
    "ionosphere = pd.read_csv(\"raw_data/ionosphere/ionosphere.data\", header=None)\n",
    "\n",
    "print(f\"Original data shape: {ionosphere.shape}\")\n",
    "\n",
    "target_col = ionosphere.columns[-1]\n",
    "class_counts = ionosphere[target_col].value_counts()\n",
    "print(f\"Class distribution:\\n{class_counts}\")\n",
    "\n",
    "class_values = class_counts.index.tolist()\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "for cls in class_values:\n",
    "    class_samples = ionosphere[ionosphere[target_col] == cls].sample(n=35, random_state=42)\n",
    "    balanced_df = pd.concat([balanced_df, class_samples])\n",
    "\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "column_names = [i for i in range(balanced_df.shape[1] - 1)] + ['target']\n",
    "balanced_df.columns = column_names\n",
    "\n",
    "if balanced_df['target'].dtype == object:\n",
    "\n",
    "    label_mapping = {label: i for i, label in enumerate(balanced_df['target'].unique())}\n",
    "    balanced_df['target'] = balanced_df['target'].map(label_mapping)\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (70, 35)\n",
      "Columns with missing values: 0\n",
      "Dropping 0 columns with >=0.0% missing values\n",
      "Dropping 3 collinear features with correlation > 0.8\n",
      "Final shape: (70, 32)\n"
     ]
    }
   ],
   "source": [
    "balanced_df = preprocess_data(balanced_df, missing_threshold=0.0, correlation_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved balanced dataset with shape: (70, 32)\n",
      "New class distribution:\n",
      "target\n",
      "0    35\n",
      "1    35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "balanced_df.to_csv(\"data/ionosphere.csv\", index=False)\n",
    "\n",
    "print(f\"Saved balanced dataset with shape: {balanced_df.shape}\")\n",
    "print(f\"New class distribution:\\n{balanced_df['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(p=0.5, n=1000, d=10, g=0.5) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates synthethic dataset\n",
    "\n",
    "    Args: \n",
    "        p: prior probability for y=1\n",
    "        n: number of instances\n",
    "        d: number of features\n",
    "        g: param for cov matrix\n",
    "\n",
    "    Returns:\n",
    "        X, y\n",
    "    \"\"\"\n",
    "    y = bernoulli.rvs(p, size=n)\n",
    "    \n",
    "    # mean vectors\n",
    "    m0 = np.zeros(d)\n",
    "    m1 = np.array([1/(i+1) for i in range(d)])\n",
    "\n",
    "    # cov matrix\n",
    "    S = np.array([[g**abs(i - j) for j in range(d)] for i in range(d)])\n",
    "\n",
    "    X = np.zeros((n, d))\n",
    "    X[y==0] = multivariate_normal.rvs(mean = m0, cov=S, size=len(X[y==0]))\n",
    "    X[y==1] = multivariate_normal.rvs(mean = m1, cov=S, size=len(X[y==1]))   \n",
    "\n",
    "    return X, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

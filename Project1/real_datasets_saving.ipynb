{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created arrhythmia.csv with shape: (452, 280)\n",
      "Column names: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', 'target']\n",
      "First few rows:\n",
      "    0  1    2   3    4    5    6    7    8    9  ...  270  271   272  273  \\\n",
      "0  75  0  190  80   91  193  371  174  121  -16  ...  0.0  9.0  -0.9  0.0   \n",
      "1  56  1  165  64   81  174  401  149   39   25  ...  0.0  8.5   0.0  0.0   \n",
      "2  54  0  172  95  138  163  386  185  102   96  ...  0.0  9.5  -2.4  0.0   \n",
      "\n",
      "   274  275  276   277   278 target  \n",
      "0  0.0  0.9  2.9  23.3  49.4      8  \n",
      "1  0.0  0.2  2.1  20.4  38.8      6  \n",
      "2  0.0  0.3  3.4  12.3  49.0     10  \n",
      "\n",
      "[3 rows x 280 columns]\n",
      "Target value distribution:\n",
      "target\n",
      "1     245\n",
      "10     50\n",
      "14      4\n",
      "15      5\n",
      "16     22\n",
      "2      44\n",
      "3      15\n",
      "4      15\n",
      "5      13\n",
      "6      25\n",
      "7       3\n",
      "8       2\n",
      "9       9\n",
      "Name: count, dtype: int64\n",
      "Sum of NaN values in table: 0\n",
      "Sum of '?' values in table: 408\n",
      "Sum of NaN values in table after replacing '?': 408\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open(\"data/arrhythmia.data\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    if line.strip():\n",
    "        values = line.strip().split(',')\n",
    "        data.append(values)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "column_names = list(range(len(df.columns)))\n",
    "df.columns = [str(i) for i in column_names]\n",
    "\n",
    "df = df.rename(columns={str(len(df.columns)-1): 'target'})\n",
    "\n",
    "df.to_csv(\"data/arrhythmia.csv\", index=False)\n",
    "\n",
    "print(f\"Created arrhythmia.csv with shape: {df.shape}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "print(f\"First few rows:\")\n",
    "print(df.head(3))\n",
    "print(f\"Target value distribution:\\n{df['target'].value_counts().sort_index()}\")\n",
    "\n",
    "print(f\"Sum of NaN values in table: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Check for '?' values\n",
    "def check_for_question_marks(df):\n",
    "    question_mark_count = (df == '?').sum().sum()\n",
    "    return question_mark_count\n",
    "question_mark_count = check_for_question_marks(df)\n",
    "print(f\"Sum of '?' values in table: {question_mark_count}\")\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "print(f\"Sum of NaN values in table after replacing '?': {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created arrhythmia.csv with shape: (452, 280)\n",
      "Column names: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', 'target']\n",
      "First few rows:\n",
      "    0  1    2   3    4    5    6    7    8    9  ...  270  271   272  273  \\\n",
      "0  75  0  190  80   91  193  371  174  121  -16  ...  0.0  9.0  -0.9  0.0   \n",
      "1  56  1  165  64   81  174  401  149   39   25  ...  0.0  8.5   0.0  0.0   \n",
      "2  54  0  172  95  138  163  386  185  102   96  ...  0.0  9.5  -2.4  0.0   \n",
      "\n",
      "   274  275  276   277   278 target  \n",
      "0  0.0  0.9  2.9  23.3  49.4      1  \n",
      "1  0.0  0.2  2.1  20.4  38.8      1  \n",
      "2  0.0  0.3  3.4  12.3  49.0      1  \n",
      "\n",
      "[3 rows x 280 columns]\n",
      "Binary target distribution:\n",
      "target\n",
      "0    245\n",
      "1    207\n",
      "Name: count, dtype: int64\n",
      "Sum of NaN values in table: 408\n"
     ]
    }
   ],
   "source": [
    "# Convert target to binary: 0 for normal (class 1), 1 for arrhythmia (classes 2-16)\n",
    "df['target'] = df['target'].apply(lambda x: 0 if x == '1' else 1)\n",
    "\n",
    "# Replace '?' with NaN to properly track missing values\n",
    "df = df.replace('?', np.nan)\n",
    "\n",
    "# Save to CSV file with binary target\n",
    "df.to_csv(\"data/arrhythmia.csv\", index=False)\n",
    "\n",
    "print(f\"Created arrhythmia.csv with shape: {df.shape}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "print(f\"First few rows:\")\n",
    "print(df.head(3))\n",
    "print(f\"Binary target distribution:\\n{df['target'].value_counts().sort_index()}\")\n",
    "print(f\"Sum of NaN values in table: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (351, 35)\n",
      "Class distribution:\n",
      "34\n",
      "g    225\n",
      "b    126\n",
      "Name: count, dtype: int64\n",
      "Label mapping: {'g': 0, 'b': 1}\n",
      "Saved balanced dataset with shape: (70, 35)\n",
      "New class distribution:\n",
      "target\n",
      "0    35\n",
      "1    35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ionosphere = pd.read_csv(\"data/ionosphere.data\", header=None)\n",
    "\n",
    "print(f\"Original data shape: {ionosphere.shape}\")\n",
    "\n",
    "target_col = ionosphere.columns[-1]\n",
    "class_counts = ionosphere[target_col].value_counts()\n",
    "print(f\"Class distribution:\\n{class_counts}\")\n",
    "\n",
    "class_values = class_counts.index.tolist()\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "for cls in class_values:\n",
    "    class_samples = ionosphere[ionosphere[target_col] == cls].sample(n=35, random_state=42)\n",
    "    balanced_df = pd.concat([balanced_df, class_samples])\n",
    "\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "column_names = [i for i in range(balanced_df.shape[1] - 1)] + ['target']\n",
    "balanced_df.columns = column_names\n",
    "\n",
    "if balanced_df['target'].dtype == object:\n",
    "\n",
    "    label_mapping = {label: i for i, label in enumerate(balanced_df['target'].unique())}\n",
    "    balanced_df['target'] = balanced_df['target'].map(label_mapping)\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "\n",
    "balanced_df.to_csv(\"data/ionosphere_balanced.csv\", index=False)\n",
    "\n",
    "print(f\"Saved balanced dataset with shape: {balanced_df.shape}\")\n",
    "print(f\"New class distribution:\\n{balanced_df['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
